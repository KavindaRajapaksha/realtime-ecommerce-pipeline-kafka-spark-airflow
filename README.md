# Real-Time E-Commerce Clickstream Analytics Pipeline

Real-time data pipeline using Kafka, Spark Structured Streaming, and Airflow to process e-commerce clickstream data with Bronze-Silver-Gold architecture.

## ğŸ—ï¸ Architecture

**Bronze Layer** â†’ Raw events in Kafka âœ…  
**Silver Layer** â†’ Cleaned Parquet files with Spark âœ…  
**Gold Layer** â†’ Aggregated metrics (Planned)

```
Producer (5 events/sec) â†’ Kafka (3 partitions) â†’ Spark Streaming â†’ Parquet Files
                           [Bronze Layer]         [Transformation]   [Silver Layer]
```



## ğŸš€ Quick Start

### Setup & Run

```cmd
# 1. Start services
docker-compose up -d

# 2. Create Kafka topic
docker exec -it kafka kafka-topics --create --topic ecom_clickstream_raw --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

# 3. Start producer (Terminal 1)
python ecom_producer.py

# 4. Start Spark job (Terminal 2)
submit_spark_job.bat
```

### Verify

```cmd
# Check Spark Master
http://localhost:8080

# Check Application UI
http://localhost:4040

# View Silver layer files
docker exec spark-master ls -R /tmp/silver_layer/
```



## ğŸ“Š Data Flow

**Producer** (`ecom_producer.py`) â†’ **Kafka** (`ecom_clickstream_raw`) â†’ **Spark Streaming** â†’ **Parquet Files** (`/tmp/silver_layer/`)

### Transformations
- Parse JSON from Kafka
- Filter nulls and invalid prices
- Deduplicate by event_id
- Add `is_high_value` flag (price > $500)
- Extract `event_date` and `event_hour`
- Partition by date and event_type

## ğŸŒ Web Interfaces

| Service | URL | Description |
|---------|-----|-------------|
| Spark Master | http://localhost:8080 | Cluster status & applications |
| Spark App | http://localhost:4040 | Streaming metrics (when job running) |
| Airflow | http://localhost:8087 | Workflow UI (planned) |

## ğŸ“‚ Project Structure

```
Final Project/
â”œâ”€â”€ docker-compose.yml          # 8 services: Zookeeper, Kafka, PostgreSQL, Airflow (3), Spark (2)
â”œâ”€â”€ ecom_producer.py            # Kafka producer (5 events/sec, 60s)
â”œâ”€â”€ spark_streaming_silver.py   # Spark job (Bronze â†’ Silver)
â”œâ”€â”€ submit_spark_job.bat        # Windows script to submit Spark job
â”œâ”€â”€ verify_silver_layer.py      # Verification script
â””â”€â”€ README.md
```

## ğŸ› ï¸ Technologies

| Technology | Version | Purpose |
|------------|---------|---------|
| Apache Kafka | 7.4.1 | Event streaming |
| Apache Spark | 3.5.0 | Stream processing |
| Apache Airflow | 2.8.3 | Orchestration (planned) |
| Docker | Latest | Containerization |
| Python | 3.8+ | Producer & scripts |

## ğŸ”§ Configuration

### Kafka
- **Topic**: `ecom_clickstream_raw`
- **Partitions**: 3
- **Bootstrap**: `localhost:9092`
- **Format**: JSON

### Spark Streaming
- **Trigger Interval**: 20 seconds
- **Checkpoint**: `/tmp/spark-checkpoints/silver`
- **Output Format**: Parquet (Snappy)
- **Partitioning**: `event_date`, `event_type`

## ğŸš¨ Troubleshooting

**Kafka connection failed?**
```cmd
docker-compose restart kafka
docker exec -it kafka kafka-broker-api-versions --bootstrap-server localhost:9092
```

**Spark UI (4040) not accessible?**
- Only available when job is running
- Check Spark Master UI (8080) for application status

**No data in Silver layer?**
```cmd
docker exec -it kafka kafka-console-consumer --topic ecom_clickstream_raw --bootstrap-server localhost:9092 --max-messages 5
docker logs spark-master
```

## ğŸ”® Next Steps

### Phase 4: Gold Layer (Planned)
- Conversion rate analytics
- High-interest, low-conversion detection
- Time-windowed aggregations

### Phase 5: Airflow DAGs (Planned)
- Automated workflows
- Scheduling & monitoring
- Alerting